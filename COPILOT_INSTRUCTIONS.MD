You are working inside a Windows-only, local-first legal evidence analysis application.
This system MUST operate fully offline and must be defensible in a legal setting.

GOAL
Build out a local “case agent” that scans a directory of sensitive evidence files,
extracts structured facts, analyzes documents, images, audio, and video,
and produces a searchable, cited knowledge base (people, events, timelines).

ABSOLUTE CONSTRAINTS
- NO cloud APIs
- NO network access
- NO modification of original evidence files
- ALWAYS preserve evidentiary provenance
- NEVER invent facts
- ALL outputs must cite source file hashes
- Unknown is acceptable; guessing is not
- Separate FACT from INFERENCE clearly

TARGET PLATFORM
- OS: Windows 10/11
- Python 3.11
- SQLite
- Optional NVIDIA GPU acceleration
- Ollama for local LLM inference

PROJECT STRUCTURE (DO NOT CHANGE)
case_agent/
├── agent/
│   ├── agent.py              # Orchestrates reasoning, querying, summarization
│   ├── prompts.py            # All LLM prompts live here
│   └── rules.md              # Non-negotiable reasoning rules
├── pipelines/
│   ├── hash_inventory.py     # SHA256 hashing & file inventory (run first)
│   ├── text_extract.py       # PDF/DOCX/TXT extraction + OCR hooks
│   ├── media_extract.py      # Audio/video extraction (FFmpeg + Whisper)
│   ├── entity_extract.py     # spaCy-based deterministic entity extraction
│   └── timeline_builder.py  # Event normalization & timeline construction
├── db/
│   ├── models.py             # SQLAlchemy models (files, text, entities, events)
│   └── init_db.py            # Initializes SQLite database
├── config.py                 # Central configuration (paths, models, intervals)
├── main.py                   # Pipeline runner / CLI entrypoint
└── requirements.txt

EVIDENCE HANDLING RULES
- Evidence directory is read-only
- Every file MUST be hashed (SHA256) before processing
- All derived data references the original file hash
- Never merge identities automatically (people, faces, speakers)
- Treat faces/speakers as abstract IDs unless manually confirmed

PIPELINE BEHAVIOR
1. hash_inventory.py
   - Walk evidence directory recursively
   - Compute SHA256
   - Store metadata in DB
2. text_extract.py
   - Extract page-level text
   - Preserve page numbers / locations
3. media_extract.py
   - Extract audio from video
   - Transcribe with Whisper (local)
   - Preserve timestamps
4. entity_extract.py
   - Use spaCy (no LLM here)
   - Extract PERSON, ORG, DATE, TIME, GPE
5. timeline_builder.py
   - Build structured events
   - Flag inferred timestamps clearly

AGENT BEHAVIOR
- The agent NEVER hallucinates
- The agent answers questions ONLY from stored evidence
- Every answer must include:
  - Evidence references
  - Confidence level
- The agent must say “insufficient evidence” when appropriate

LLM USAGE (LOCAL ONLY)
- Use Ollama-compatible models (Mistral, LLaMA)
- Output STRICT JSON when extracting facts
- Prompts live ONLY in agent/prompts.py

WHAT TO BUILD NEXT
- Flesh out missing pipeline implementations
- Add error handling and logging
- Improve event extraction logic
- Add a query interface that returns cited results
- Keep everything deterministic and auditable


## Environment setup (development)

1. Install Python 3.11 (Windows) — prefer the official Python 3.11 installer or use winget:
   - winget install --id Python.Python.3.11 -e
2. Create the project virtualenv under the project root (this repository uses this path):
   - python -m venv C:\\Projects\\FileAnalyzer\\.venv
3. Activate and install Python dependencies inside the venv:
   - C:\\Projects\\FileAnalyzer\\.venv\\Scripts\\python.exe -m pip install --upgrade pip setuptools wheel
   - C:\\Projects\\FileAnalyzer\\.venv\\Scripts\\python.exe -m pip install -r requirements.txt
   - C:\\Projects\\FileAnalyzer\\.venv\\Scripts\\python.exe -m spacy download en_core_web_sm
4. System binaries (installed via winget or manually):
   - FFmpeg (for audio/video processing): winget install --id Gyan.FFmpeg -e
   - Tesseract-OCR (for OCR): winget install --id tesseract-ocr.tesseract -e

Additional notes:
- The media pipeline now persists transcriptions into the `transcriptions` table (see `case_agent/db/models.py::Transcription`).
- Transcriptions are generated by local Whisper (if installed). When Whisper is unavailable, a deterministic empty transcription is persisted so pipeline behavior remains auditable and deterministic.
- Unit tests for media transcription (mocking Whisper) are in `tests/test_media_extract.py` and pass locally.
5. After installing, configure paths if necessary in `case_agent/config.py` (FFMPEG_PATH, TESSERACT_CMD) to point to full executables for determinism.
6. Run a smoke pipeline:
   - Create a test evidence file in `evidence/` (the folder will be created automatically)
   - Run: C:\\Projects\\FileAnalyzer\\.venv\\Scripts\\python.exe -m case_agent.main C:\\Projects\\FileAnalyzer\\evidence --db C:\\Projects\\FileAnalyzer\\file_analyzer.db

Notes:
- This project targets Python 3.11 for compatibility with spaCy and related libs.
- Installing system binaries may require admin rights and/or a shell restart to refresh PATH.
- Keep evidence immutable; tests may write to an isolated `evidence` folder for smoke tests.

Logging & error handling
- A central logging setup is available in `case_agent/logging_config.py`.
- `case_agent/main.py` calls `setup_logging()` on startup.
- Pipelines now catch and log errors per-file to avoid stopping a full pipeline run; they persist deterministic empty outputs where appropriate (e.g., empty transcriptions when Whisper fails).

CI-friendly helpers
- A PowerShell test runner is at `scripts/run_tests.ps1` to run tests under the project venv.
- Add this script to your CI job to run unit tests in a Windows environment.


DO NOT:
- Add cloud services
- Summarize without citations
- Auto-identify real people
- Modify original files
- Collapse fact and inference

When unsure, prefer correctness and traceability over convenience.
